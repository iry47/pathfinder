{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import IS_PUNCT, LOWER\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Request & Determine Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_text = 'Hi, I would like to travel this winter and go skiing. Normally I will go from paris to grenoble to ski at my favorite resort!'\n",
    "fr_text = 'Bonjour, je m\\'appelle Ryan et j\\'aimerais voyager cet hivers et faire du ski. Normalement je j\\irai à Lucelle depuis Paris pour arriver chez ma station préférée !'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Text:  False\n",
      "French Text:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"English Text: \", is_french(eng_text))\n",
    "print(\"French Text: \", is_french(fr_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Detect if text is French\n",
    "\"\"\"\n",
    "def is_french(text):\n",
    "    return 'fr' == detect(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Departure and Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOC  |  Bonjour\n",
      "PER  |  Ryan\n",
      "LOC  |  Paris\n"
     ]
    }
   ],
   "source": [
    "# must download french package with :\n",
    "# python -m spacy download fr_core_news_sm\n",
    "doc = nlp(fr_text)\n",
    "for entity in doc.ents:\n",
    "    print(entity.label_, ' | ', entity.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy Matcher trained with Geonames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanheadley/.pyenv/versions/3.7.9/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3170: DtypeWarning: Columns (9,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PandasArray>\n",
       "[                                    'Col de Recon',\n",
       "                                          'Lucelle',\n",
       "                            'Les Cornettes de Bise',\n",
       "                                        'Lertzbach',\n",
       "                                  'Le Cheval Blanc',\n",
       "                                         'Jougnena',\n",
       "                                           'London',\n",
       "                                       'Wolfesberg',\n",
       "                                       'Saar River',\n",
       "                                         'Rosselle',\n",
       " ...\n",
       "               'Abbaye de Saint-Florent-lès-Saumur',\n",
       "                           'Abbatiale Saint-Pierre',\n",
       "           'Ancienne Abbaye Saint-Pierre de Corbie',\n",
       "             \"Site archéologique d'Alba-la-Romaine\",\n",
       "                    'Église Saint-Hilaire le Grand',\n",
       "                               'Abbaye Saint-Winoc',\n",
       "               'Église Saint-Mathias de Barbezieux',\n",
       " 'Basilique Saint-Étienne de Neuvy-Saint-Sépulchre',\n",
       "                                  'Parc du chàteau',\n",
       "                             'Église Saint-Cyprien']\n",
       "Length: 167884, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use Geonames file to train spaCy Matcher\n",
    "fr_cities = pd.read_csv('data/FR_villes.txt', sep=\"\\t\", header=None)\n",
    "fr_cities[1].array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skillPattern(skill):\n",
    "    pattern = []\n",
    "    for b in skill.split():\n",
    "        pattern.append({'LOWER':b})  \n",
    "    return pattern\n",
    "\n",
    "def buildPatterns(skills):\n",
    "    pattern = []\n",
    "    for skill in skills:\n",
    "        pattern.append(skillPattern(skill))\n",
    "    return list(zip(skills, pattern))\n",
    "def on_match(matcher, doc, id, matches):\n",
    "    return matches\n",
    "\n",
    "def buildMatcher(patterns):\n",
    "    name = \"\"\n",
    "    list_dict = []\n",
    "    for pattern in patterns:\n",
    "        name += pattern[0]\n",
    "        list_dict.append(pattern[1])    \n",
    "    matcher.add(name, list_dict)\n",
    "    return matcher\n",
    "    \n",
    "def cityMatcher(matcher, text):\n",
    "    skills = []\n",
    "    doc = nlp(text.lower())\n",
    "    matches = matcher(doc)\n",
    "    for b in matches:\n",
    "        match_id, start, end = b\n",
    "        print(doc[start : end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [ 'paris',\n",
    "'grenoble',\n",
    "'kanpur',\n",
    "'noida',\n",
    "'ghaziabad',\n",
    "'chennai',\n",
    "'hydrabad',\n",
    "'luckhnow',\n",
    "'saharanpur',\n",
    "'dehradun',\n",
    "'bombay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = buildPatterns(fr_cities[1].array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Lucelle', [{'LOWER': 'Lucelle'}])\n",
      "167884\n"
     ]
    }
   ],
   "source": [
    "print(patterns[1])\n",
    "print(len(patterns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_matcher = buildMatcher(patterns)\n",
    "len(city_matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cityMatcher(city_matcher, fr_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b17d7f78fb2f9e06d34c9e83e9ea991ed874ac5a2ce6642113047182b5845c21"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('3.7.9': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
