{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from langdetect import detect\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import IS_PUNCT, LOWER\n",
    "import spacy\n",
    "from gensim.models import word2vec\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Request & Determine Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_text = 'Hi, I would like to travel this winter and go skiing. Normally I will go from paris to grenoble to ski at my favorite resort!'\n",
    "fr_text = 'Bonjour, je m\\'appelle Ryan et j\\'aimerais voyager cet hivers et faire du ski. Normalement je j\\irai à Lucelle depuis Paris pour arriver chez ma station préférée !'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Detect if text is French\n",
    "\"\"\"\n",
    "def is_french(text):\n",
    "    return 'fr' == detect(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Text:  False\n",
      "French Text:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"English Text: \", is_french(eng_text))\n",
    "print(\"French Text: \", is_french(fr_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Departure and Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOC  |  Bonjour\n",
      "PER  |  Ryan\n",
      "LOC  |  Lucelle\n",
      "LOC  |  Paris\n"
     ]
    }
   ],
   "source": [
    "# must download french package with :\n",
    "# python -m spacy download fr_core_news_sm\n",
    "doc = nlp(fr_text)\n",
    "for entity in doc.ents:\n",
    "    print(entity.label_, ' | ', entity.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy Matcher trained with Geonames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PandasArray>\n",
       "[                                    'Col de Recon',\n",
       "                                          'Lucelle',\n",
       "                            'Les Cornettes de Bise',\n",
       "                                        'Lertzbach',\n",
       "                                  'Le Cheval Blanc',\n",
       "                                         'Jougnena',\n",
       "                                           'London',\n",
       "                                       'Wolfesberg',\n",
       "                                       'Saar River',\n",
       "                                         'Rosselle',\n",
       " ...\n",
       "               'Abbaye de Saint-Florent-lès-Saumur',\n",
       "                           'Abbatiale Saint-Pierre',\n",
       "           'Ancienne Abbaye Saint-Pierre de Corbie',\n",
       "             \"Site archéologique d'Alba-la-Romaine\",\n",
       "                    'Église Saint-Hilaire le Grand',\n",
       "                               'Abbaye Saint-Winoc',\n",
       "               'Église Saint-Mathias de Barbezieux',\n",
       " 'Basilique Saint-Étienne de Neuvy-Saint-Sépulchre',\n",
       "                                  'Parc du chàteau',\n",
       "                             'Église Saint-Cyprien']\n",
       "Length: 167884, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use Geonames file to train spaCy Matcher\n",
    "fr_cities = pd.read_csv('data/FR_villes.txt', sep=\"\\t\", header=None)\n",
    "fr_cities[1].array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skillPattern(skill):\n",
    "    pattern = []\n",
    "    for b in skill.split():\n",
    "        pattern.append({'LOWER':b})  \n",
    "    return pattern\n",
    "\n",
    "def buildPatterns(skills):\n",
    "    pattern = []\n",
    "    for skill in skills:\n",
    "        pattern.append(skillPattern(skill))\n",
    "    return list(zip(skills, pattern))\n",
    "def on_match(matcher, doc, id, matches):\n",
    "    return matches\n",
    "\n",
    "def buildMatcher(patterns):\n",
    "    name = \"\"\n",
    "    list_dict = []\n",
    "    for pattern in patterns:\n",
    "        name += pattern[0]\n",
    "        list_dict.append(pattern[1])    \n",
    "    matcher.add(name, list_dict)\n",
    "    return matcher\n",
    "    \n",
    "def cityMatcher(matcher, text):\n",
    "    skills = []\n",
    "    doc = nlp(text.lower())\n",
    "    matches = matcher(doc)\n",
    "    for b in matches:\n",
    "        match_id, start, end = b\n",
    "        print(doc[start : end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [ 'paris',\n",
    "'grenoble',\n",
    "'kanpur',\n",
    "'noida',\n",
    "'ghaziabad',\n",
    "'chennai',\n",
    "'hydrabad',\n",
    "'luckhnow',\n",
    "'saharanpur',\n",
    "'dehradun',\n",
    "'bombay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = buildPatterns(fr_cities[1].array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Lucelle', [{'LOWER': 'Lucelle'}])\n",
      "167884\n"
     ]
    }
   ],
   "source": [
    "print(patterns[1])\n",
    "print(len(patterns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.matcher.matcher.Matcher object at 0x17f43d050>\n"
     ]
    }
   ],
   "source": [
    "city_matcher = buildMatcher(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "len(city_matcher)\n",
    "print(cityMatcher(city_matcher, fr_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour',\n",
       " ',',\n",
       " 'je',\n",
       " \"m'appelle\",\n",
       " 'Ryan',\n",
       " 'et',\n",
       " \"j'aimerais\",\n",
       " 'voyager',\n",
       " 'cet',\n",
       " 'hivers',\n",
       " 'et',\n",
       " 'faire',\n",
       " 'du',\n",
       " 'ski',\n",
       " '.',\n",
       " 'Normalement',\n",
       " 'je',\n",
       " 'j\\\\irai',\n",
       " 'à',\n",
       " 'Lucelle',\n",
       " 'depuis',\n",
       " 'Paris',\n",
       " 'pour',\n",
       " 'arriver',\n",
       " 'chez',\n",
       " 'ma',\n",
       " 'station',\n",
       " 'préférée',\n",
       " '!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize \n",
    "text_tokenized = word_tokenize(fr_text)\n",
    "text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bonjour',\n",
       " ',',\n",
       " 'je',\n",
       " \"m'appel\",\n",
       " 'ryan',\n",
       " 'et',\n",
       " \"j'aimerai\",\n",
       " 'voyag',\n",
       " 'cet',\n",
       " 'hiver',\n",
       " 'et',\n",
       " 'fair',\n",
       " 'du',\n",
       " 'ski',\n",
       " '.',\n",
       " 'normal',\n",
       " 'je',\n",
       " 'j\\\\irai',\n",
       " 'à',\n",
       " 'lucel',\n",
       " 'depui',\n",
       " 'pari',\n",
       " 'pour',\n",
       " 'arriv',\n",
       " 'chez',\n",
       " 'ma',\n",
       " 'station',\n",
       " 'préférée',\n",
       " '!']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "text_stem = [porter.stem(word) for word in text_tokenized]\n",
    "text_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(\n",
    "    [text_stem],\n",
    "    window=20,\n",
    "    min_count=2,\n",
    "    workers=1\n",
    ")\n",
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'et': 0, 'je': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = model.wv.key_to_index\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 161)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(fr_text, total_examples=1,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'Voyager en train de lille à lyon',\n",
    "    'Les trains sont mieux. J\\'irai de Lille à Lyon',\n",
    "    'A toulon et prendre un bus à marseille',\n",
    "    'A toulon et prendre un avion à marseille',\n",
    "    'A toulon et marcher à marseille',\n",
    "    'Manger des fruits',\n",
    "    'Nager a la plage'    \n",
    "]\n",
    "fr_text = ['Je veux prendre un train de paris à lyon']\n",
    "\n",
    "test_sentences_embeddings = model.encode(test_sentences)\n",
    "real_sentence_embedding = model.encode(fr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7666555 , 0.79194474, 0.6420541 , 0.60242796, 0.49063894,\n",
       "        0.34201652, 0.41831568]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(\n",
    "    [real_sentence_embedding[0]],\n",
    "    test_sentences_embeddings[0:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prendre un paragraph et renvoyer les endroits\n",
    "SI il existe une demande de transport\n",
    "'''\n",
    "def extract_cities(sentences):\n",
    "    # model must already be loaded\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    similarities = cosine_similarity(\n",
    "        [real_sentence_embedding[0]],\n",
    "        sentence_embeddings\n",
    "    )\n",
    "    biggest_number = max(similarities[0])\n",
    "    if biggest_number < 0.75:\n",
    "        return \"SPAM\"\n",
    "    best_sentence_ind = np.where(similarities[0] == biggest_number)\n",
    "    best_sentence = sentences[best_sentence_ind[0][0]]\n",
    "    \n",
    "    return cityMatcher(city_matcher, best_sentence)\n",
    "    \n",
    "print(extract_cities(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b17d7f78fb2f9e06d34c9e83e9ea991ed874ac5a2ce6642113047182b5845c21"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('3.7.9': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
