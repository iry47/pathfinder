{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component One: Voice Recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import sounddevice\n",
    "from scipy.io.wavfile import write\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Record Voice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component will record an audio sample, and save it to an audio file.\n",
    "\n",
    "### Process\n",
    "1. Run the cell below\n",
    "2. Input a number for the amount of time you want the recorder to run\n",
    "3. (Maybe) grant access for the use of your devices microphone\n",
    "4. Speak\n",
    "5. The cell will automatically stop running after the time is completed\n",
    "\n",
    "### Details\n",
    "- The number of channels is specific for your device\n",
    "- The function creates a new file or rewrites the existing one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording.....n\n",
      "Finished.....nPlease check your output file\n"
     ]
    }
   ],
   "source": [
    "fs= 44100\n",
    "second =  int(input(\"Enter time duration in seconds: \"))\n",
    "print(\"Recording.....n\")\n",
    "record_voice = sounddevice.rec( int ( second * fs ) , samplerate = fs , channels = 1, dtype=np.int16 ) # might be different depending on machine\n",
    "sounddevice.wait()\n",
    "write(\"audio_file.wav\",fs,record_voice)\n",
    "print(\"Finished.....nPlease check your output file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Transcribe Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component takes an audio file and writes its content to a text file\n",
    "\n",
    "### Process\n",
    "1. Download the vosk model. The linto french model is suggested\n",
    "2. Run the cell below\n",
    "\n",
    "### Details\n",
    "- the model must be downloaded and have the appropriate name\n",
    "- the file names are provided at the beginning of the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"linto\"\n",
    "audio_file = './audio_file.wav'\n",
    "text_file = 'transcription.txt'\n",
    "\n",
    "model_path = \"../../app/back/models/{}\".format(model_name)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Please download the model from https://alphacephei.com/vosk/models and unpack as 'model' in the current folder.\")\n",
    "    exit(1)\n",
    "\n",
    "wf = wave.open(audio_file, \"rb\")\n",
    "\n",
    "\n",
    "if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
    "    print (\"Audio file must be WAV format mono PCM.\")\n",
    "    exit(1)\n",
    "\n",
    "model = Model(model_path)\n",
    "rec = KaldiRecognizer(model, wf.getframerate())\n",
    "# rec.SetMaxAlternatives(10)\n",
    "# rec.SetWords(True)\n",
    "\n",
    "result = []\n",
    "while True:\n",
    "    data = wf.readframes(4000)\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "    if rec.AcceptWaveform(data):\n",
    "        result.append(json.loads(rec.Result()))\n",
    "\n",
    "# ret = [sentence[\"alternatives\"][0][\"text\"] for sentence in result]\n",
    "ret = result[0][\"text\"]\n",
    "\n",
    "with open(text_file, 'w') as file:\n",
    "    file.write(ret)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Two: Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component takes a text file and extracts a travel request destination and departure\n",
    "\n",
    "### Process\n",
    "1. Run the cell below\n",
    "\n",
    "### Details\n",
    "- the model must be downloaded and have the appropriate name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'departure': 'paris', 'destination': 'marseille'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file = 'transcription.txt'\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "words_before_departure = ['de', 'depuis', 'provenance']\n",
    "words_before_destination = ['à', 'a', 'en', 'jusqu\\'a', 'vers', 'par']\n",
    "example_travel_sentence = ['Je veux prendre un train de paris à lyon']\n",
    "\n",
    "def get_cities(sentence):\n",
    "    \"\"\" Take a sentence and return all cities within\n",
    "\n",
    "    Args:\n",
    "        sentence (str): any sentence\n",
    "\n",
    "    Returns:\n",
    "        Array: A list of cities\n",
    "    \"\"\"\n",
    "    cities = []\n",
    "    doc = nlp(sentence)\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == \"LOC\":\n",
    "            cities.append(entity.text)\n",
    "    \n",
    "    return cities\n",
    "\n",
    "def check_for_travel_request(sentences):\n",
    "    \"\"\" Take a list of sentences and return the sentence\n",
    "        containing a request to travel by train\n",
    "\n",
    "    Args:\n",
    "        sentences (Array<str>): List of sentences\n",
    "\n",
    "    Returns:\n",
    "        str: travel request sentence or SPAM\n",
    "    \"\"\"\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    real_sentence_embedding = model.encode(example_travel_sentence)\n",
    "    similarities = cosine_similarity(\n",
    "        [real_sentence_embedding[0]],\n",
    "        sentence_embeddings\n",
    "    )\n",
    "    biggest_number = max(similarities[0])\n",
    "    if biggest_number < 0.75:\n",
    "        return False\n",
    "    best_sentence_ind = np.where(similarities[0] == biggest_number)\n",
    "    return sentences[best_sentence_ind[0][0]]\n",
    "\n",
    "def get_destination_and_departure():\n",
    "    \"\"\"Takes the pre-defined text file and determines \n",
    "    the destination and departure\n",
    "\n",
    "    Returns:\n",
    "        dict: of destination and departure \n",
    "        OR False if text not valid\n",
    "    \"\"\"\n",
    "    # read the text file\n",
    "    file = open(text_file, \"r\")\n",
    "    sentences = file.read()\n",
    "    file.close()\n",
    "    if '.' in sentences:\n",
    "        sentences.split('.')\n",
    "    else:\n",
    "        sentences = [sentences]\n",
    "\n",
    "    if detect(sentences[0]) != 'fr':\n",
    "        return False\n",
    "    \n",
    "    # check for travel request\n",
    "    request = check_for_travel_request(sentences)\n",
    "    if not request:\n",
    "        return False\n",
    "    \n",
    "    # get destination and departure\n",
    "    departure = []\n",
    "    destination = []\n",
    "    \n",
    "    cities = get_cities(request)\n",
    "    words = word_tokenize(request)\n",
    "    for city in cities:\n",
    "        index = words.index(city)\n",
    "        if index == 0: continue\n",
    "        if words[index-1] in words_before_departure: departure.append(city)\n",
    "        elif words[index-1] in words_before_destination: destination.append(city)\n",
    "    \n",
    "    return {\n",
    "        \"departure\": departure[0],\n",
    "        \"destination\": destination[0]\n",
    "    }\n",
    "    \n",
    "get_destination_and_departure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Three: Pathfinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component takes a departure and destination, and calculates the fastest path between the nearest train stations to these locations.\n",
    "\n",
    "### Process\n",
    "1. Run the cell below\n",
    "...\n",
    "\n",
    "### Details\n",
    "- ..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
